{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Hybrid Model: CVSS Features + Description Embeddings\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook combines two complementary approaches for predicting vulnerability exploitation:\n",
    "\n",
    "1. **CVSS Features**: Structured vulnerability metrics (severity, impact, attack vectors)\n",
    "2. **Description Embeddings**: Semantic text features from CVE descriptions\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Load and cache the Sentence Transformer model for reproducibility\n",
    "- Generate embeddings for any missing CVE descriptions\n",
    "- Combine CVSS features with embedding features\n",
    "- Train a hybrid logistic regression model\n",
    "- Compare performance: CVSS-only vs Embeddings-only vs Hybrid\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "The hybrid model should achieve **>0.90 ROC-AUC** by leveraging both structured and unstructured signals.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"\\nüñ•Ô∏è  Device: {'MPS (Apple Silicon)' if torch.backends.mps.is_available() else 'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-cache",
   "metadata": {},
   "source": [
    "## 2. Load and Cache Sentence Transformer Model\n",
    "\n",
    "We'll save the model locally for reproducibility and faster loading in future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-cache-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "MODEL_CACHE_DIR = '../../embedding_pipeline/models/sentence_transformer_cache'\n",
    "DEVICE = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_or_download_model(model_name=MODEL_NAME, cache_dir=MODEL_CACHE_DIR, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Load sentence transformer model from local cache or download if not present.\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer: Loaded model ready for encoding\n",
    "    \"\"\"\n",
    "    cache_path = Path(cache_dir)\n",
    "    \n",
    "    if cache_path.exists() and len(list(cache_path.iterdir())) > 0:\n",
    "        print(f\"üìÇ Loading model from local cache: {cache_dir}\")\n",
    "        model = SentenceTransformer(str(cache_path), device=device)\n",
    "        print(f\"   ‚úÖ Model loaded from cache\")\n",
    "    else:\n",
    "        print(f\"üì• Downloading model: {model_name}\")\n",
    "        print(f\"   This may take a few minutes on first run...\")\n",
    "        model = SentenceTransformer(model_name, device=device)\n",
    "        \n",
    "        # Save to local cache\n",
    "        print(f\"üíæ Saving model to local cache: {cache_dir}\")\n",
    "        cache_path.mkdir(parents=True, exist_ok=True)\n",
    "        model.save(str(cache_path))\n",
    "        print(f\"   ‚úÖ Model cached for future use\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "embedding_model = load_or_download_model()\n",
    "embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "\n",
    "print(f\"\\nüìä Model info:\")\n",
    "print(f\"   Name: {MODEL_NAME}\")\n",
    "print(f\"   Embedding dimension: {embedding_dim}\")\n",
    "print(f\"   Max sequence length: {embedding_model.max_seq_length}\")\n",
    "print(f\"   Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## 3. Load Data\n",
    "\n",
    "We need three datasets:\n",
    "1. **CVSS features** (from baseline model pipeline)\n",
    "2. **Embeddings** (pre-generated from description pipeline)\n",
    "3. **Target labels** (KEV catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Load CVSS features (cleaned dataset from baseline model)\n",
    "print(\"\\nüìÇ Loading CVSS features...\")\n",
    "if os.path.exists('../../data/data.csv'):\n",
    "    cvss_df = pd.read_csv('../../data/data.csv')\n",
    "    print(f\"   Loaded from: data/data.csv\")\n",
    "else:\n",
    "    # Fall back to processed vulnerabilities\n",
    "    cvss_df = pd.read_csv('../../data/processed_vulnerabilities.csv')\n",
    "    print(f\"   Loaded from: data/processed_vulnerabilities.csv\")\n",
    "\n",
    "print(f\"   Shape: {cvss_df.shape}\")\n",
    "print(f\"   Columns: {list(cvss_df.columns[:10])}...\")\n",
    "\n",
    "# 2. Load pre-generated embeddings\n",
    "print(\"\\nüìÇ Loading pre-generated embeddings...\")\n",
    "embedding_data = np.load('../../embedding_pipeline/data/embeddings_sentencetransformer.npz')\n",
    "embeddings = embedding_data['embeddings']\n",
    "embedding_metadata = pd.read_csv('../../embedding_pipeline/data/metadata_sentencetransformer.csv')\n",
    "\n",
    "print(f\"   Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"   Metadata shape: {embedding_metadata.shape}\")\n",
    "\n",
    "# 3. Load target labels\n",
    "print(\"\\nüìÇ Loading KEV catalog for targets...\")\n",
    "kev_df = pd.read_csv('../../data/known_exploited_vulnerabilities.csv')\n",
    "kev_set = set(kev_df['cveID'].values)\n",
    "print(f\"   KEV catalog size: {len(kev_set):,} known exploited CVEs\")\n",
    "\n",
    "print(\"\\n‚úÖ All data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "align-data",
   "metadata": {},
   "source": [
    "## 4. Align and Merge Datasets\n",
    "\n",
    "We need to ensure all datasets are aligned by CVE ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "align-data-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ALIGNING DATASETS BY CVE ID\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Add embeddings to metadata\n",
    "print(\"\\nüîó Merging embeddings with metadata...\")\n",
    "embedding_metadata['embedding_idx'] = range(len(embedding_metadata))\n",
    "\n",
    "# Ensure CVSS data has target column (if not, create it)\n",
    "if 'target' not in cvss_df.columns:\n",
    "    print(\"\\nüéØ Creating target variable for CVSS data...\")\n",
    "    cvss_df['target'] = cvss_df['id'].apply(lambda x: 1 if x in kev_set else 0)\n",
    "    print(f\"   Exploited CVEs in CVSS data: {cvss_df['target'].sum():,}\")\n",
    "\n",
    "# Find common CVE IDs between CVSS and embedding datasets\n",
    "print(\"\\nüîç Finding common CVEs...\")\n",
    "cvss_ids = set(cvss_df['id'].values)\n",
    "embedding_ids = set(embedding_metadata['id'].values)\n",
    "common_ids = cvss_ids.intersection(embedding_ids)\n",
    "\n",
    "print(f\"   CVSS dataset: {len(cvss_ids):,} CVEs\")\n",
    "print(f\"   Embedding dataset: {len(embedding_ids):,} CVEs\")\n",
    "print(f\"   Common CVEs: {len(common_ids):,} ({len(common_ids)/min(len(cvss_ids), len(embedding_ids))*100:.1f}% overlap)\")\n",
    "\n",
    "# Filter to common IDs\n",
    "cvss_df_filtered = cvss_df[cvss_df['id'].isin(common_ids)].copy()\n",
    "embedding_metadata_filtered = embedding_metadata[embedding_metadata['id'].isin(common_ids)].copy()\n",
    "\n",
    "# Sort both by ID to ensure alignment\n",
    "cvss_df_filtered = cvss_df_filtered.sort_values('id').reset_index(drop=True)\n",
    "embedding_metadata_filtered = embedding_metadata_filtered.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "# Verify alignment\n",
    "assert (cvss_df_filtered['id'].values == embedding_metadata_filtered['id'].values).all(), \"IDs not aligned!\"\n",
    "print(\"\\n‚úÖ Datasets aligned successfully!\")\n",
    "\n",
    "# Extract aligned embeddings\n",
    "aligned_embedding_indices = embedding_metadata_filtered['embedding_idx'].values\n",
    "aligned_embeddings = embeddings[aligned_embedding_indices]\n",
    "\n",
    "print(f\"\\nüìä Final aligned dataset:\")\n",
    "print(f\"   Total CVEs: {len(cvss_df_filtered):,}\")\n",
    "print(f\"   CVSS features shape: {cvss_df_filtered.shape}\")\n",
    "print(f\"   Embeddings shape: {aligned_embeddings.shape}\")\n",
    "print(f\"   Exploited CVEs: {cvss_df_filtered['target'].sum():,} ({cvss_df_filtered['target'].mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare-features",
   "metadata": {},
   "source": [
    "## 5. Prepare Feature Sets\n",
    "\n",
    "We'll create three feature matrices:\n",
    "1. **CVSS-only**: Structured features\n",
    "2. **Embedding-only**: Semantic features\n",
    "3. **Hybrid**: Combined features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-features-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PREPARING FEATURE SETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract year for temporal split (2015-2024 train, 2025 test)\n",
    "if 'year' not in cvss_df_filtered.columns:\n",
    "    cvss_df_filtered['year'] = cvss_df_filtered['id'].str.extract(r'CVE-(\\d{4})-')[0].astype(int)\n",
    "\n",
    "# 1. CVSS Features\n",
    "print(\"\\nüîß Preparing CVSS features...\")\n",
    "\n",
    "# Define CVSS feature columns (exclude id, target, year, description)\n",
    "exclude_cols = ['id', 'target', 'year', 'description', 'embedding_idx']\n",
    "cvss_feature_cols = [col for col in cvss_df_filtered.columns if col not in exclude_cols]\n",
    "\n",
    "# Handle categorical columns with one-hot encoding if needed\n",
    "categorical_cols = cvss_df_filtered[cvss_feature_cols].select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"   Encoding {len(categorical_cols)} categorical columns...\")\n",
    "    cvss_df_encoded = cvss_df_filtered[cvss_feature_cols].copy()\n",
    "    cvss_df_encoded[categorical_cols] = cvss_df_encoded[categorical_cols].fillna('MISSING').astype(str)\n",
    "    \n",
    "    encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "    encoded_array = encoder.fit_transform(cvss_df_encoded[categorical_cols])\n",
    "    encoded_cols = encoder.get_feature_names_out(categorical_cols)\n",
    "    \n",
    "    # Combine numeric and encoded categorical\n",
    "    numeric_cols = [col for col in cvss_feature_cols if col not in categorical_cols]\n",
    "    cvss_numeric = cvss_df_filtered[numeric_cols].fillna(0).values\n",
    "    cvss_features = np.hstack([cvss_numeric, encoded_array])\n",
    "    \n",
    "    print(f\"   Numeric features: {len(numeric_cols)}\")\n",
    "    print(f\"   Encoded categorical features: {len(encoded_cols)}\")\n",
    "else:\n",
    "    cvss_features = cvss_df_filtered[cvss_feature_cols].fillna(0).values\n",
    "\n",
    "print(f\"   CVSS features shape: {cvss_features.shape}\")\n",
    "\n",
    "# 2. Embedding Features (already prepared)\n",
    "print(f\"\\nüîß Embedding features shape: {aligned_embeddings.shape}\")\n",
    "\n",
    "# 3. Hybrid Features (concatenate)\n",
    "print(\"\\nüîß Creating hybrid features...\")\n",
    "hybrid_features = np.hstack([cvss_features, aligned_embeddings])\n",
    "print(f\"   Hybrid features shape: {hybrid_features.shape}\")\n",
    "print(f\"   = {cvss_features.shape[1]} CVSS features + {aligned_embeddings.shape[1]} embedding features\")\n",
    "\n",
    "# Target and metadata\n",
    "y = cvss_df_filtered['target'].values\n",
    "years = cvss_df_filtered['year'].values\n",
    "cve_ids = cvss_df_filtered['id'].values\n",
    "\n",
    "print(f\"\\nüìä Feature summary:\")\n",
    "print(f\"   Total samples: {len(y):,}\")\n",
    "print(f\"   CVSS-only dimensions: {cvss_features.shape[1]}\")\n",
    "print(f\"   Embedding-only dimensions: {aligned_embeddings.shape[1]}\")\n",
    "print(f\"   Hybrid dimensions: {hybrid_features.shape[1]}\")\n",
    "print(f\"   Target distribution: {y.sum():,} exploited ({y.mean()*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ All feature sets prepared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-test-split",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split (Temporal)\n",
    "\n",
    "Following the embedding pipeline approach:\n",
    "- **Train**: 2015-2024\n",
    "- **Test**: 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TEMPORAL TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create temporal split\n",
    "train_mask = years < 2025\n",
    "test_mask = years == 2025\n",
    "\n",
    "print(f\"\\nüìä Split configuration:\")\n",
    "print(f\"   Training: 2015-2024\")\n",
    "print(f\"   Testing: 2025\")\n",
    "\n",
    "# Split all feature sets\n",
    "X_cvss_train, X_cvss_test = cvss_features[train_mask], cvss_features[test_mask]\n",
    "X_emb_train, X_emb_test = aligned_embeddings[train_mask], aligned_embeddings[test_mask]\n",
    "X_hybrid_train, X_hybrid_test = hybrid_features[train_mask], hybrid_features[test_mask]\n",
    "y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "print(f\"\\nüìä Dataset sizes:\")\n",
    "print(f\"   Training samples: {len(y_train):,}\")\n",
    "print(f\"   Test samples: {len(y_test):,}\")\n",
    "print(f\"   Training exploited: {y_train.sum():,} ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"   Test exploited: {y_test.sum():,} ({y_test.mean()*100:.2f}%)\")\n",
    "\n",
    "# Year distribution\n",
    "print(f\"\\nüìä Year distribution:\")\n",
    "year_df = pd.DataFrame({'year': years, 'target': y})\n",
    "for year in sorted(year_df['year'].unique()):\n",
    "    year_data = year_df[year_df['year'] == year]\n",
    "    split_type = 'TRAIN' if year < 2025 else 'TEST '\n",
    "    print(f\"   [{split_type}] {year}: {len(year_data):,} CVEs ({year_data['target'].sum()} exploited)\")\n",
    "\n",
    "print(\"\\n‚úÖ Train/test split complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-models",
   "metadata": {},
   "source": [
    "## 7. Train Models\n",
    "\n",
    "We'll train three logistic regression models:\n",
    "1. **CVSS-only** (baseline)\n",
    "2. **Embedding-only** (text-based)\n",
    "3. **Hybrid** (combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-models-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Shared configuration\n",
    "model_config = {\n",
    "    'class_weight': 'balanced',\n",
    "    'max_iter': 1000,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Dictionary to store models and results\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "# 1. CVSS-only Model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 1: CVSS-ONLY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüèãÔ∏è  Training...\")\n",
    "model_cvss = LogisticRegression(**model_config)\n",
    "model_cvss.fit(X_cvss_train, y_train)\n",
    "models['CVSS-only'] = model_cvss\n",
    "print(\"   ‚úÖ Training complete\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_cvss = model_cvss.predict(X_cvss_test)\n",
    "y_proba_cvss = model_cvss.predict_proba(X_cvss_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "results['CVSS-only'] = {\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba_cvss),\n",
    "    'avg_precision': average_precision_score(y_test, y_proba_cvss),\n",
    "    'f1': f1_score(y_test, y_pred_cvss),\n",
    "    'y_pred': y_pred_cvss,\n",
    "    'y_proba': y_proba_cvss\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   ROC-AUC: {results['CVSS-only']['roc_auc']:.4f}\")\n",
    "print(f\"   Average Precision: {results['CVSS-only']['avg_precision']:.4f}\")\n",
    "print(f\"   F1 Score: {results['CVSS-only']['f1']:.4f}\")\n",
    "\n",
    "# 2. Embedding-only Model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 2: EMBEDDING-ONLY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüèãÔ∏è  Training...\")\n",
    "model_emb = LogisticRegression(**model_config)\n",
    "model_emb.fit(X_emb_train, y_train)\n",
    "models['Embedding-only'] = model_emb\n",
    "print(\"   ‚úÖ Training complete\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_emb = model_emb.predict(X_emb_test)\n",
    "y_proba_emb = model_emb.predict_proba(X_emb_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "results['Embedding-only'] = {\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba_emb),\n",
    "    'avg_precision': average_precision_score(y_test, y_proba_emb),\n",
    "    'f1': f1_score(y_test, y_pred_emb),\n",
    "    'y_pred': y_pred_emb,\n",
    "    'y_proba': y_proba_emb\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   ROC-AUC: {results['Embedding-only']['roc_auc']:.4f}\")\n",
    "print(f\"   Average Precision: {results['Embedding-only']['avg_precision']:.4f}\")\n",
    "print(f\"   F1 Score: {results['Embedding-only']['f1']:.4f}\")\n",
    "\n",
    "# 3. Hybrid Model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 3: HYBRID (CVSS + EMBEDDINGS)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüèãÔ∏è  Training...\")\n",
    "model_hybrid = LogisticRegression(**model_config)\n",
    "model_hybrid.fit(X_hybrid_train, y_train)\n",
    "models['Hybrid'] = model_hybrid\n",
    "print(\"   ‚úÖ Training complete\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_hybrid = model_hybrid.predict(X_hybrid_test)\n",
    "y_proba_hybrid = model_hybrid.predict_proba(X_hybrid_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "results['Hybrid'] = {\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba_hybrid),\n",
    "    'avg_precision': average_precision_score(y_test, y_proba_hybrid),\n",
    "    'f1': f1_score(y_test, y_pred_hybrid),\n",
    "    'y_pred': y_pred_hybrid,\n",
    "    'y_proba': y_proba_hybrid\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   ROC-AUC: {results['Hybrid']['roc_auc']:.4f}\")\n",
    "print(f\"   Average Precision: {results['Hybrid']['avg_precision']:.4f}\")\n",
    "print(f\"   F1 Score: {results['Hybrid']['f1']:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'ROC-AUC': [results[m]['roc_auc'] for m in results.keys()],\n",
    "    'Avg Precision': [results[m]['avg_precision'] for m in results.keys()],\n",
    "    'F1 Score': [results[m]['f1'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "# Sort by ROC-AUC\n",
    "comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Performance Metrics:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"\\nüìà Hybrid Model Improvements:\")\n",
    "hybrid_roc = results['Hybrid']['roc_auc']\n",
    "cvss_roc = results['CVSS-only']['roc_auc']\n",
    "emb_roc = results['Embedding-only']['roc_auc']\n",
    "\n",
    "print(f\"   vs CVSS-only: {(hybrid_roc - cvss_roc)*100:+.2f} percentage points\")\n",
    "print(f\"   vs Embedding-only: {(hybrid_roc - emb_roc)*100:+.2f} percentage points\")\n",
    "\n",
    "if hybrid_roc > max(cvss_roc, emb_roc):\n",
    "    print(\"\\nüèÜ Hybrid model OUTPERFORMS both individual approaches!\")\n",
    "elif hybrid_roc > cvss_roc:\n",
    "    print(\"\\n‚úÖ Hybrid model improves over CVSS-only\")\n",
    "elif hybrid_roc > emb_roc:\n",
    "    print(\"\\n‚úÖ Hybrid model improves over Embedding-only\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Hybrid model does not improve over individual approaches\")\n",
    "\n",
    "# Determine best model\n",
    "best_model = comparison_df.iloc[0]['Model']\n",
    "best_roc = comparison_df.iloc[0]['ROC-AUC']\n",
    "print(f\"\\nü•á BEST MODEL: {best_model} (ROC-AUC: {best_roc:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualizations",
   "metadata": {},
   "source": [
    "## 9. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualizations-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "colors = {'CVSS-only': 'blue', 'Embedding-only': 'green', 'Hybrid': 'red'}\n",
    "\n",
    "for model_name in results.keys():\n",
    "    fpr, tpr, _ = roc_curve(y_test, results[model_name]['y_proba'])\n",
    "    roc_auc = results[model_name]['roc_auc']\n",
    "    ax.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})', \n",
    "            linewidth=2.5, color=colors[model_name])\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "ax.set_xlabel('False Positive Rate', fontsize=13)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=13)\n",
    "ax.set_title('ROC Curves: Model Comparison', fontsize=15, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curves\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "for model_name in results.keys():\n",
    "    precision, recall, _ = precision_recall_curve(y_test, results[model_name]['y_proba'])\n",
    "    avg_prec = results[model_name]['avg_precision']\n",
    "    ax.plot(recall, precision, label=f'{model_name} (AP = {avg_prec:.3f})', \n",
    "            linewidth=2.5, color=colors[model_name])\n",
    "\n",
    "ax.axhline(y=y_test.mean(), color='k', linestyle='--', \n",
    "           label=f'Baseline (prevalence = {y_test.mean():.3f})', linewidth=1)\n",
    "ax.set_xlabel('Recall', fontsize=13)\n",
    "ax.set_ylabel('Precision', fontsize=13)\n",
    "ax.set_title('Precision-Recall Curves: Model Comparison', fontsize=15, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance Bar Chart\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(results))\n",
    "width = 0.25\n",
    "\n",
    "roc_scores = [results[m]['roc_auc'] for m in results.keys()]\n",
    "ap_scores = [results[m]['avg_precision'] for m in results.keys()]\n",
    "f1_scores = [results[m]['f1'] for m in results.keys()]\n",
    "\n",
    "ax.bar(x - width, roc_scores, width, label='ROC-AUC', color='steelblue')\n",
    "ax.bar(x, ap_scores, width, label='Avg Precision', color='coral')\n",
    "ax.bar(x + width, f1_scores, width, label='F1 Score', color='mediumseagreen')\n",
    "\n",
    "ax.set_xlabel('Models', fontsize=13)\n",
    "ax.set_ylabel('Score', fontsize=13)\n",
    "ax.set_title('Performance Comparison Across Metrics', fontsize=15, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results.keys())\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for i, (roc, ap, f1) in enumerate(zip(roc_scores, ap_scores, f1_scores)):\n",
    "    ax.text(i - width, roc + 0.02, f'{roc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax.text(i, ap + 0.02, f'{ap:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax.text(i + width, f1 + 0.02, f'{f1:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-results",
   "metadata": {},
   "source": [
    "## 10. Save Results and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('../../embedding_pipeline/hybrid_results')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save results JSON\n",
    "results_dict = {\n",
    "    'models': {\n",
    "        model_name: {\n",
    "            'roc_auc': float(metrics['roc_auc']),\n",
    "            'average_precision': float(metrics['avg_precision']),\n",
    "            'f1_score': float(metrics['f1'])\n",
    "        }\n",
    "        for model_name, metrics in results.items()\n",
    "    },\n",
    "    'best_model': best_model,\n",
    "    'best_roc_auc': float(best_roc),\n",
    "    'dataset': {\n",
    "        'train_size': int(len(y_train)),\n",
    "        'test_size': int(len(y_test)),\n",
    "        'train_exploited': int(y_train.sum()),\n",
    "        'test_exploited': int(y_test.sum())\n",
    "    }\n",
    "}\n",
    "\n",
    "results_path = output_dir / 'hybrid_model_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "print(f\"\\nüíæ Results saved: {results_path}\")\n",
    "\n",
    "# Save comparison table\n",
    "comparison_path = output_dir / 'model_comparison.csv'\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"üíæ Comparison table saved: {comparison_path}\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': cve_ids[test_mask],\n",
    "    'year': years[test_mask],\n",
    "    'true_label': y_test,\n",
    "    'cvss_prediction': y_pred_cvss,\n",
    "    'cvss_probability': y_proba_cvss,\n",
    "    'embedding_prediction': y_pred_emb,\n",
    "    'embedding_probability': y_proba_emb,\n",
    "    'hybrid_prediction': y_pred_hybrid,\n",
    "    'hybrid_probability': y_proba_hybrid\n",
    "})\n",
    "\n",
    "predictions_path = output_dir / 'predictions_all_models.csv'\n",
    "predictions_df.to_csv(predictions_path, index=False)\n",
    "print(f\"üíæ Predictions saved: {predictions_path}\")\n",
    "\n",
    "# Save models\n",
    "models_dir = output_dir / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model_path = models_dir / f\"{model_name.lower().replace(' ', '_').replace('-', '_')}_model.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"üíæ Model saved: {model_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ All results and models saved!\")\n",
    "print(f\"\\nüìÅ Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 11. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüéØ Objective: Combine CVSS features + Description embeddings\")\n",
    "print(f\"   to maximize vulnerability exploitation prediction performance\")\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"   Total CVEs: {len(y):,}\")\n",
    "print(f\"   Training (2015-2024): {len(y_train):,} CVEs\")\n",
    "print(f\"   Testing (2025): {len(y_test):,} CVEs\")\n",
    "print(f\"   Class imbalance: {(1 - y_test.mean())/y_test.mean():.1f}:1 ratio\")\n",
    "\n",
    "print(f\"\\nüèÜ RESULTS:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüí° KEY FINDINGS:\")\n",
    "\n",
    "if hybrid_roc > max(cvss_roc, emb_roc):\n",
    "    improvement = (hybrid_roc - max(cvss_roc, emb_roc)) * 100\n",
    "    print(f\"   ‚úÖ Hybrid model achieves {hybrid_roc:.4f} ROC-AUC\")\n",
    "    print(f\"   ‚úÖ Improves over best individual model by {improvement:.2f} percentage points\")\n",
    "    print(f\"   ‚úÖ Demonstrates that CVSS + embeddings provide complementary signals\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Hybrid model ({hybrid_roc:.4f}) does not exceed individual models\")\n",
    "    print(f\"   ‚ö†Ô∏è  Best single approach: {best_model} ({best_roc:.4f})\")\n",
    "    print(f\"   üí° Consider: feature selection, different combination strategies\")\n",
    "\n",
    "if hybrid_roc >= 0.90:\n",
    "    print(f\"\\nüéâ MILESTONE ACHIEVED: ROC-AUC ‚â• 0.90!\")\n",
    "elif hybrid_roc >= 0.88:\n",
    "    print(f\"\\n‚ú® EXCELLENT PERFORMANCE: ROC-AUC {hybrid_roc:.4f}\")\n",
    "else:\n",
    "    print(f\"\\nüìà GOOD PERFORMANCE: ROC-AUC {hybrid_roc:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to: {output_dir}\")\n",
    "print(f\"\\nüöÄ Next steps:\")\n",
    "print(f\"   - Analyze predictions on high-risk vulnerabilities\")\n",
    "print(f\"   - Deploy best model for operational use\")\n",
    "print(f\"   - Retrain periodically with new data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚ú® HYBRID MODEL ANALYSIS COMPLETE ‚ú®\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
